[
  {
    "objectID": "lab-06.html",
    "href": "lab-06.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(tidymodels)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(purrr)\nlibrary(kernlab)\n\n\nAttaching package: 'kernlab'\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nlibrary(rsample)\n\n# Download data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Download metadata and documentation\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', mode = \"wb\")\n\n# Get data specific text files\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Construct URLs and file names for the data\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('data/camels_{types}.txt')\n\n# Download specific data\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab-06.html#lab-activity",
    "href": "lab-06.html#lab-activity",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lab Activity:",
    "text": "Lab Activity:\n\n# Model Preparation\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Create a scatter plot of aridity vs rainfall with log axes\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Scale the legend to the log scale plot\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Building the Model\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab-06.html#question-3-deliverable-adjusted-wf-set",
    "href": "lab-06.html#question-3-deliverable-adjusted-wf-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 3 Deliverable: Adjusted WF set",
    "text": "Question 3 Deliverable: Adjusted WF set\n\n# Data Validation\n# prep %&gt;% bake %&gt;% predict\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n# Model Evaluation\n  #Statistical\n  metrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n  # Visual\n  ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n# Alternative Method: Workflow\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Adding other models to the workflow\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# Adding xgboost\nxg_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\") \n\nxg_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(xg_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nxg_data &lt;- augment(xg_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\n# Adding an nnet\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\") \n\nnn_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(nn_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nnn_data &lt;- augment(nn_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Workflowset approach\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xg_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.552  0.0292    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0270    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.564  0.0255    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0259    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, nn_model, xg_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nThe bagged MLP, neural network model appears to be the best with a mean r-sq of ~0.78 and a rmse of 0.57. The rsme is not the lowest but the r-sq is better than the rand forest at ~0.77. If rsme was a priority I would go with the rf model."
  },
  {
    "objectID": "lab-06.html#question-4-deliverable",
    "href": "lab-06.html#question-4-deliverable",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 4 Deliverable",
    "text": "Question 4 Deliverable\n\n# Set seed\nset.seed(6515)\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean))   # Add logQmean column to df\n\n# Data Splitting\n## Generate split (75/25)\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n  ## Extract training and testing sets\n  camels_tr &lt;- training(camels_split)\n  camels_te  &lt;- testing(camels_split)\n  ## 10-fold CV dataset\n  camels_10cv &lt;- vfold_cv(camels_tr, v = 10)\n  \n# Recipe Vars Review\n  # Check for skewing\ncamels_tr %&gt;%\n  select(pet_mean, p_mean, runoff_ratio, baseflow_index, aridity, slope_mean, area_geospa_fabric) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Recipie\nalt_rec &lt;- recipe(logQmean ~ pet_mean + p_mean + aridity + runoff_ratio + baseflow_index + slope_mean + area_geospa_fabric, data = camels_tr) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_interact(terms = ~ pet_mean:p_mean + aridity:runoff_ratio + area_geospa_fabric:slope_mean) %&gt;% \n  step_corr(all_predictors(), threshold = 0.9) %&gt;%   # Remove highly correlated predictors to avoid multicollinearity.\n  step_normalize(all_predictors()) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n  \n# Define and Train Models\n  ## Define rf model\n  rf_alt_model &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  rf_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(rf_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n   \n  rf_predictions &lt;- augment(rf_alt_wf, new_data = camels_te) \n\n  ## Define xg model\n  xg_alt_model &lt;- boost_tree() %&gt;% \n    set_engine(\"xgboost\") %&gt;% \n    set_mode(\"regression\")\n  \n  xg_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(xg_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  xg_predictions &lt;- augment(xg_alt_wf, new_data = camels_te)\n  \n  ## Define nueral net model\n  nn_alt_model &lt;- bag_mlp() %&gt;% \n    set_engine(\"nnet\") %&gt;% \n    set_mode(\"regression\")\n  \n  nn_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(nn_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  nn_predictions &lt;- augment(nn_alt_wf, new_data = camels_te)\n  \n  ## Define linear reg model\n  lm_alt_model &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;% \n    set_mode(\"regression\")\n  \n  lm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(lm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  lm_predictions &lt;- augment(lm_alt_wf, new_data = camels_te) \n  \n  ## Define SVM-nonlinear model\n  svm_alt_model &lt;- svm_rbf() %&gt;% \n    set_engine(\"kernlab\") %&gt;% \n    set_mode(\"regression\")\n\n  svm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(svm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)  \n  \n  svm_predictions &lt;- augment(svm_alt_wf, new_data = camels_te)\n  \n # Implement workflowset analysis\n  \n  alt_wf_set &lt;- workflow_set(preproc = list(rec),\n                          models = list(rf = rf_alt_model, \n                                        xg = xg_alt_model, \n                                        nn = nn_alt_model, \n                                        lm = lm_alt_model, \n                                        svm = svm_alt_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_10cv) \n  \nautoplot(alt_wf_set)\n\n\n\n\n\n\n\nrank_results(alt_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 10 × 9\n   wflow_id   .config       .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 recipe_nn  Preprocessor… rmse    0.565  0.0347    10 recipe       bag_…     1\n 2 recipe_nn  Preprocessor… rsq     0.771  0.0149    10 recipe       bag_…     1\n 3 recipe_lm  Preprocessor… rmse    0.571  0.0426    10 recipe       line…     2\n 4 recipe_lm  Preprocessor… rsq     0.765  0.0222    10 recipe       line…     2\n 5 recipe_svm Preprocessor… rmse    0.561  0.0414    10 recipe       svm_…     3\n 6 recipe_svm Preprocessor… rsq     0.760  0.0204    10 recipe       svm_…     3\n 7 recipe_rf  Preprocessor… rmse    0.576  0.0397    10 recipe       rand…     4\n 8 recipe_rf  Preprocessor… rsq     0.754  0.0188    10 recipe       rand…     4\n 9 recipe_xg  Preprocessor… rmse    0.610  0.0371    10 recipe       boos…     5\n10 recipe_xg  Preprocessor… rsq     0.724  0.0161    10 recipe       boos…     5\n\n# Moving forward with the NN Model\n\n  ## Extract the model coefficients\n  nn_coeff &lt;- coef(nn_alt_model)  \n  nn_coeff\n\nNULL\n\n  ## Use the data to make predictions\n  metrics(nn_predictions, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0615\n2 rsq     standard      0.998 \n3 mae     standard      0.0111\n\n  ggplot(nn_predictions, aes(x = logQmean, y = .pred)) +\n    geom_point(aes(color = .pred), size = 2) +\n    scale_color_gradient(low = \"tan\", high = \"royalblue\") +\n    labs(title = \"Observed vs Predicted Values with the NN Model\",\n         x = \"Observed Log Mean Flow\",\n         y = \"Predicted Log Mean Flow\",\n         color = \"Aridity\") +\n    geom_abline(linetype = 2) +\n    theme_linedraw()\n\n\n\n\n\n\n\n\nQ4b: I chose a complex formula to attempt to compute multiple elements of the watershed. Temperature (pet), precipitation (p) and aridity are all related as is runoff, slope and catchment area (area_geo…). Finally I included baseflow index because it seems like predicting flow will be challenging without first understanding what water is already there from groundwater sources, other inputs, etc.\nQ4c: I used selected the above models in an attempt to find non-linear representations for the data. Given the complexity of the formula I made for my recipe I anticipated the data fitting linear models poorly.\nQ4e: For the recipe I created, the bag_mlp neural network model performed the best. It had the highest mean r-squared (~0.771) and the second lowest root mean standard error (rmse) (~0.565). The SVM model had a marginally lower rmse (~0.561) but also had a lower mean r-squared (~0.0760). If I really wanted to I could tune the SVM and NN models to optimize them and then compare them again.\nQ4f: I am very happy with the results, outside of a few outliers, my models seems to have improve on the performance of the recipe and models from part 3."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Lab 8: Tuning ML Models of Hydrological Data",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.3.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.2\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(workflowsets)\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'ggfortify':\n  method          from   \n  autoplot.glmnet parsnip\n\nlibrary(parsnip)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n# Data Import/Tidy/Transform    \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id') \n\n# Add log(q_mean) to df\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean)) %&gt;% \n  mutate(across(everything(), as.double))\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(everything(), as.double)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n59\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n59\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngauge_id\n0\n1.00\n6265830.84\n3976867.52\n1013500.00\n2370650.00\n6278300.00\n9382765.00\n14400000.00\n▇▃▅▃▃\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nhigh_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nlow_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ngeol_1st_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\ngeol_2nd_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\ndom_land_cover\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nlogQmean\n1\n1.00\n-0.11\n1.17\n-5.39\n-0.46\n0.12\n0.56\n2.27\n▁▁▂▇▂\n\n\n\n\nvis_dat(camels)\n\n\n\n\n\n\n\n\n\n# Set seed\nset.seed(567)\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_tr &lt;- training(camels_split)\ncamels_te  &lt;- testing(camels_split)\n\n# Cross-validation folds\ncamels_10cv &lt;- vfold_cv(camels_tr, v = 10)\n\n# Recipe\nrec &lt;- recipe(logQmean ~ pet_mean + p_mean + aridity + runoff_ratio + baseflow_index + slope_mean + area_geospa_fabric, data = camels_tr) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_interact(terms = ~ pet_mean:p_mean + aridity:runoff_ratio + area_geospa_fabric:slope_mean) %&gt;% \n  step_corr(all_predictors(), threshold = 0.9) %&gt;%   # Remove highly correlated predictors to avoid multicollinearity.\n  step_normalize(all_predictors()) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n# Define and Train Models\n  ## Define rf model\n  rf_model &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  rf_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(rf_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n   \n  rf_predictions &lt;- augment(rf_wf, new_data = camels_te) \n\n  ## Define xg model\n  xg_model &lt;- boost_tree() %&gt;% \n    set_engine(\"xgboost\") %&gt;% \n    set_mode(\"regression\")\n  \n  xg_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(xg_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  xg_predictions &lt;- augment(xg_wf, new_data = camels_te)\n  \n  ## Define nueral net model\n  nn_model &lt;- bag_mlp() %&gt;% \n    set_engine(\"nnet\") %&gt;% \n    set_mode(\"regression\")\n  \n  nn_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(nn_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  nn_predictions &lt;- augment(nn_wf, new_data = camels_te)\n  \n  ## Define linear reg model\n  lm_model &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;% \n    set_mode(\"regression\")\n  \n  lm_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(lm_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  lm_predictions &lt;- augment(lm_wf, new_data = camels_te) \n  \n  # Implement workflowset analysis\n  \n  ml_wf_set &lt;- workflow_set(preproc = list(rec),\n                          models = list(rf = rf_model, \n                                        xg = xg_model, \n                                        nn = nn_model, \n                                        lm = lm_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_10cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(ml_wf_set)\n\n\n\n\n\n\n\nrank_results(ml_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id  .config        .metric   mean std_err     n preprocessor model  rank\n  &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn Preprocessor1… rmse    0.0304 1.04e-2    10 recipe       bag_…     1\n2 recipe_nn Preprocessor1… rsq     0.999  4.79e-4    10 recipe       bag_…     1\n3 recipe_xg Preprocessor1… rmse    0.122  1.61e-2    10 recipe       boos…     2\n4 recipe_xg Preprocessor1… rsq     0.989  2.04e-3    10 recipe       boos…     2\n5 recipe_rf Preprocessor1… rmse    0.175  2.43e-2    10 recipe       rand…     3\n6 recipe_rf Preprocessor1… rsq     0.981  3.14e-3    10 recipe       rand…     3\n7 recipe_lm Preprocessor1… rmse    0.217  2.40e-2    10 recipe       line…     4\n8 recipe_lm Preprocessor1… rsq     0.967  4.71e-3    10 recipe       line…     4\n\n\n\n# model tuning\ntuned_nn_model &lt;- bag_mlp(\n  hidden_units = tune(), \n  penalty = tune()\n) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf_tune &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;% \n  add_model(tuned_nn_model)\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\n\n# define search space\nmy.grid &lt;- grid_space_filling(dials, size = 20)\n\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_10cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\nautoplot(model_params)\n\n\n\n\n\n\n\ncollect_metrics(model_params)\n\n# A tibble: 60 × 8\n   hidden_units       penalty .metric .estimator   mean     n std_err .config   \n          &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1            1 0.000000144   mae     standard   0.104     10 0.00738 Preproces…\n 2            1 0.000000144   rmse    standard   0.173     10 0.0246  Preproces…\n 3            1 0.000000144   rsq     standard   0.980     10 0.00397 Preproces…\n 4            1 0.0000616     mae     standard   0.109     10 0.00715 Preproces…\n 5            1 0.0000616     rmse    standard   0.174     10 0.0242  Preproces…\n 6            1 0.0000616     rsq     standard   0.979     10 0.00393 Preproces…\n 7            2 0.0264        mae     standard   0.0399    10 0.00453 Preproces…\n 8            2 0.0264        rmse    standard   0.0824    10 0.0205  Preproces…\n 9            2 0.0264        rsq     standard   0.995     10 0.00191 Preproces…\n10            2 0.00000000113 mae     standard   0.0339    10 0.00442 Preproces…\n# ℹ 50 more rows\n\nbest_mae &lt;- show_best(model_params, metric = \"mae\", n = 1)\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n#&gt; The first row shows the mean MAE across resamples, SE of the MAE estimate, # of resamples, and mean SE. Penalty is the best hyperparameter set for this model. \n\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)\nfinal_fit &lt;- last_fit(final_wf, split = camels_split)\nfinal_metrics &lt;- collect_metrics(final_fit)\n\n# DOTHIS:: CHANGE THESE NUUMBERS the final model's rmse 53.5% and the rsq is 78%. This means that 78% of the variance is explained by the model. This is a decent number. The rmse is the average prediction error, and this percentage is above 50% which is a pretty high error amount. This model is reasonably good, but the rmse suggests plenty of room for improvement. \n\npredictions &lt;- collect_predictions(model_params)\n\nggplot(predictions, aes(x = .pred, y = logQmean)) +\n  geom_smooth(method = lm, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  scale_color_gradient() +\n  labs(\n    title = \"Actual vs. Predicted Values\", \n    x = \"Predicted\", \n    y = \"Actual\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfinal_fit_full &lt;- fit(final_wf, data = camels)\naugmented_preds &lt;- augment(final_fit_full, new_data = camels)\n\naugmented_preds &lt;- augmented_preds %&gt;% \n  mutate(residual_sq = (logQmean - .pred)^2)\n\nmap_preds &lt;- ggplot(augmented_preds, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +\n  scale_color_viridis_c(name = \"Predicted\") +\n  coord_fixed() +\n  labs(title = \"Map of Predicted logQmean\") +\n  theme_minimal()\n\nmap_resid &lt;- ggplot(augmented_preds, aes(x = .pred, y = residual_sq)) +\n  geom_point() +\n  scale_color_viridis_c(name = \"Residual²\") +\n  labs(title = \"Map of Squared Residuals\") +\n  theme_minimal()\n\nmaps_combined &lt;- map_preds | map_resid\n\nprint(maps_combined)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(tidymodels)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(purrr)\nlibrary(kernlab)\n\n\nAttaching package: 'kernlab'\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nlibrary(rsample)\n\n# Download data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Download metadata and documentation\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', mode = \"wb\")\n\n# Get data specific text files\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Construct URLs and file names for the data\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('data/camels_{types}.txt')\n\n# Download specific data\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "index.html#lab-activity",
    "href": "index.html#lab-activity",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lab Activity:",
    "text": "Lab Activity:\n\n# Model Preparation\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Create a scatter plot of aridity vs rainfall with log axes\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Scale the legend to the log scale plot\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Building the Model\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "index.html#question-3-deliverable-adjusted-wf-set",
    "href": "index.html#question-3-deliverable-adjusted-wf-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 3 Deliverable: Adjusted WF set",
    "text": "Question 3 Deliverable: Adjusted WF set\n\n# Data Validation\n# prep %&gt;% bake %&gt;% predict\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n# Model Evaluation\n  #Statistical\n  metrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n  # Visual\n  ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n# Alternative Method: Workflow\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Adding other models to the workflow\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# Adding xgboost\nxg_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\") \n\nxg_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(xg_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nxg_data &lt;- augment(xg_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\n# Adding an nnet\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\") \n\nnn_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(nn_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nnn_data &lt;- augment(nn_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Workflowset approach\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xg_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.552  0.0292    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0270    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.564  0.0255    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0259    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, nn_model, xg_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nThe bagged MLP, neural network model appears to be the best with a mean r-sq of ~0.78 and a rmse of 0.57. The rsme is not the lowest but the r-sq is better than the rand forest at ~0.77. If rsme was a priority I would go with the rf model."
  },
  {
    "objectID": "index.html#question-4-deliverable",
    "href": "index.html#question-4-deliverable",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 4 Deliverable",
    "text": "Question 4 Deliverable\n\n# Set seed\nset.seed(6515)\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean))   # Add logQmean column to df\n\n# Data Splitting\n## Generate split (75/25)\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n  ## Extract training and testing sets\n  camels_tr &lt;- training(camels_split)\n  camels_te  &lt;- testing(camels_split)\n  ## 10-fold CV dataset\n  camels_10cv &lt;- vfold_cv(camels_tr, v = 10)\n  \n# Recipe Vars Review\n  # Check for skewing\ncamels_tr %&gt;%\n  select(pet_mean, p_mean, runoff_ratio, baseflow_index, aridity, slope_mean, area_geospa_fabric) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Recipie\nalt_rec &lt;- recipe(logQmean ~ pet_mean + p_mean + aridity + runoff_ratio + baseflow_index + slope_mean + area_geospa_fabric, data = camels_tr) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_interact(terms = ~ pet_mean:p_mean + aridity:runoff_ratio + area_geospa_fabric:slope_mean) %&gt;% \n  step_corr(all_predictors(), threshold = 0.9) %&gt;%   # Remove highly correlated predictors to avoid multicollinearity.\n  step_normalize(all_predictors()) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n  \n# Define and Train Models\n  ## Define rf model\n  rf_alt_model &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  rf_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(rf_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n   \n  rf_predictions &lt;- augment(rf_alt_wf, new_data = camels_te) \n\n  ## Define xg model\n  xg_alt_model &lt;- boost_tree() %&gt;% \n    set_engine(\"xgboost\") %&gt;% \n    set_mode(\"regression\")\n  \n  xg_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(xg_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  xg_predictions &lt;- augment(xg_alt_wf, new_data = camels_te)\n  \n  ## Define nueral net model\n  nn_alt_model &lt;- bag_mlp() %&gt;% \n    set_engine(\"nnet\") %&gt;% \n    set_mode(\"regression\")\n  \n  nn_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(nn_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  nn_predictions &lt;- augment(nn_alt_wf, new_data = camels_te)\n  \n  ## Define linear reg model\n  lm_alt_model &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;% \n    set_mode(\"regression\")\n  \n  lm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(lm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  lm_predictions &lt;- augment(lm_alt_wf, new_data = camels_te) \n  \n  ## Define SVM-nonlinear model\n  svm_alt_model &lt;- svm_rbf() %&gt;% \n    set_engine(\"kernlab\") %&gt;% \n    set_mode(\"regression\")\n\n  svm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(svm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)  \n  \n  svm_predictions &lt;- augment(svm_alt_wf, new_data = camels_te)\n  \n # Implement workflowset analysis\n  \n  alt_wf_set &lt;- workflow_set(preproc = list(rec),\n                          models = list(rf = rf_alt_model, \n                                        xg = xg_alt_model, \n                                        nn = nn_alt_model, \n                                        lm = lm_alt_model, \n                                        svm = svm_alt_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_10cv) \n  \nautoplot(alt_wf_set)\n\n\n\n\n\n\n\nrank_results(alt_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 10 × 9\n   wflow_id   .config       .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 recipe_nn  Preprocessor… rmse    0.565  0.0347    10 recipe       bag_…     1\n 2 recipe_nn  Preprocessor… rsq     0.771  0.0149    10 recipe       bag_…     1\n 3 recipe_lm  Preprocessor… rmse    0.571  0.0426    10 recipe       line…     2\n 4 recipe_lm  Preprocessor… rsq     0.765  0.0222    10 recipe       line…     2\n 5 recipe_svm Preprocessor… rmse    0.561  0.0414    10 recipe       svm_…     3\n 6 recipe_svm Preprocessor… rsq     0.760  0.0204    10 recipe       svm_…     3\n 7 recipe_rf  Preprocessor… rmse    0.576  0.0397    10 recipe       rand…     4\n 8 recipe_rf  Preprocessor… rsq     0.754  0.0188    10 recipe       rand…     4\n 9 recipe_xg  Preprocessor… rmse    0.610  0.0371    10 recipe       boos…     5\n10 recipe_xg  Preprocessor… rsq     0.724  0.0161    10 recipe       boos…     5\n\n# Moving forward with the NN Model\n\n  ## Extract the model coefficients\n  nn_coeff &lt;- coef(nn_alt_model)  \n  nn_coeff\n\nNULL\n\n  ## Use the data to make predictions\n  metrics(nn_predictions, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0615\n2 rsq     standard      0.998 \n3 mae     standard      0.0111\n\n  ggplot(nn_predictions, aes(x = logQmean, y = .pred)) +\n    geom_point(aes(color = .pred), size = 2) +\n    scale_color_gradient(low = \"tan\", high = \"royalblue\") +\n    labs(title = \"Observed vs Predicted Values with the NN Model\",\n         x = \"Observed Log Mean Flow\",\n         y = \"Predicted Log Mean Flow\",\n         color = \"Aridity\") +\n    geom_abline(linetype = 2) +\n    theme_linedraw()\n\n\n\n\n\n\n\n\nQ4b: I chose a complex formula to attempt to compute multiple elements of the watershed. Temperature (pet), precipitation (p) and aridity are all related as is runoff, slope and catchment area (area_geo…). Finally I included baseflow index because it seems like predicting flow will be challenging without first understanding what water is already there from groundwater sources, other inputs, etc.\nQ4c: I used selected the above models in an attempt to find non-linear representations for the data. Given the complexity of the formula I made for my recipe I anticipated the data fitting linear models poorly.\nQ4e: For the recipe I created, the bag_mlp neural network model performed the best. It had the highest mean r-squared (~0.771) and the second lowest root mean standard error (rmse) (~0.565). The SVM model had a marginally lower rmse (~0.561) but also had a lower mean r-squared (~0.0760). If I really wanted to I could tune the SVM and NN models to optimize them and then compare them again.\nQ4f: I am very happy with the results, outside of a few outliers, my models seems to have improve on the performance of the recipe and models from part 3."
  }
]